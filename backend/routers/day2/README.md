# Summerschool 2025 - Tag 2: Reasoning-Techniken

## üéØ Lernziele

1.  **Self-Consistency**: Die Zuverl√§ssigkeit von Modellantworten durch mehrfaches "Nachfragen" und einen Mehrheitsentscheid verbessern.
2.  **Tool Use**: Einem LLM beibringen, externe Werkzeuge (hier: einen Taschenrechner) per Function Calling zu nutzen, um seine eigenen F√§higkeiten zu erweitern.
3.  **Plan-and-Solve**: Komplexe Anfragen in kleinere, l√∂sbare Schritte zerlegen zu lassen, bevor die endg√ºltige Antwort generiert wird.

---

## üöÄ Deine Aufgaben

### Core Task 1: Self-Consistency bei Rechenaufgaben (ca. 30 min)

Teste, ob ein LLM bei einfachen Rechen- oder Logikaufgaben zuverl√§ssiger wird, wenn man es mehrfach antworten l√§sst und die h√§ufigste Antwort ausw√§hlt.

**Anforderungen:**
-   **Eingabe**: Eine Frage, die logisches Denken oder Kopfrechnen erfordert (z.B. "Wenn ein Zug um 14:00 Uhr startet und 2,5 Stunden braucht, wann kommt er an?").
-   **Logik**:
    1.  Stelle dieselbe Frage 5 Mal an das LLM (mit einer Temperatur > 0.5, um Varianz zu erzeugen).
    2.  Sammle die 5 Antworten.
    3.  Ermittle die h√§ufigste Antwort (Mehrheitsvotum).
    4.  Gib die Mehrheits-Antwort als finales Ergebnis zur√ºck.
-   **Implementierung**:
    -   Implementiere die Logik im `echo` Endpoint in `backend/routers/day2.py`.
    -   Vergleiche f√ºr 2-3 Beispielfragen das Ergebnis eines einzelnen Aufrufs mit dem Ergebnis der Self-Consistency-Methode.

### Core Task 2: Tool Use - Der Taschenrechner (ca. 30 min)

Implementiere einen einfachen Taschenrechner als externes "Tool", das das LLM bei Bedarf nutzen kann.

**Anforderungen:**
-   **Tool-Definition**: Definiere eine Funktion (z.B. `calculate(expression: str)`), die mathematische Ausdr√ºcke auswerten kann.
-   **Function Calling**:
    -   Erstelle ein "Function Calling"-Schema, das dem LLM die `calculate`-Funktion beschreibt (Parameter, Zweck).
    -   Formuliere einen Prompt, der das LLM anweist, bei mathematischen Berechnungen in einer Anfrage zuerst die Funktion aufzurufen. Beispiel-Prompt: "Was sind 3 √Ñpfel plus 5 Orangen, und was kostet das bei 2‚Ç¨ pro Frucht?"
    -   Das LLM soll den Tool-Call `calculate("8 * 2")` generieren.
-   **Implementierung**:
    -   Integriere die Function-Calling-Logik in den `echo` Endpoint. Die Antwort an den User soll das Ergebnis der Berechnung enthalten.

### Core Task 3: Plan-and-Solve (ca. 15 min)

Lasse das LLM bei mehrstufigen Anfragen zuerst einen Plan erstellen und diesen dann abarbeiten.

**Anforderungen:**
-   **Eingabe**: Eine Aufgabe, die mehrere Schritte erfordert (z.B. "Wandle 5 Meilen in Meter um und runde auf die zweite Nachkommastelle").
-   **Logik**:
    1.  **Planungs-Schritt**: Sende einen ersten Prompt an das LLM, der es anweist, nur einen Plan in Stichpunkten zu erstellen.
        -   *Ergebnis*: `["1. Umrechnungsfaktor Meilen->km holen", "2. km->m umrechnen", "3. Ergebnis runden"]`
    2.  **Ausf√ºhrungs-Schritt**: Kombiniere den urspr√ºnglichen Prompt mit dem generierten Plan und bitte das LLM, die finale Antwort zu generieren.
-   **Implementierung**:
    -   Implementiere diesen zweistufigen Prozess. Logge sowohl den Plan als auch die finale Antwort, um den Prozess nachzuvollziehen.

---

### ‚≠ê Stretch Goals (f√ºr die Schnellen)

-   **Step-Back-Prompting**: Implementiere eine Variante, bei der das LLM vor der finalen Antwort eine allgemeine "Step-Back"-Frage generieren und beantworten muss (z.B. "Was ist das √ºbergeordnete Prinzip hinter der Einheitenumrechnung?"). Vergleiche die Antwortqualit√§t.
-   **Metriken**: Miss und vergleiche f√ºr alle Core Tasks die Latenz (Zeit) und die Token-Kosten. Erstelle eine kleine Tabelle.

---

##  deliverables

-   F√ºge deine Ergebnisse (Code-Snippets, Vergleichstabellen, geloggte Pl√§ne) in diese `README.md` ein.
-   Sei bereit, deine Erkenntnisse am Ende des Labs zu teilen: Wo hat welche Technik am besten funktioniert?
